# Nextest configuration for api_ollama
#
# This configuration addresses integration test resource exhaustion while maintaining
# strict "fail loudly" behavior per codebase requirements (issue-server-exhaustion-001).
#
# Integration tests require extended timeouts and resource limits because they:
# 1. Each test binary spawns its own Ollama server on unique port (11435-11534)
# 2. Pull AI models (tinyllama, etc.) on first run (~1GB RAM + model data)
# 3. Perform end-to-end API validation against live Ollama services
# 4. Running 6+ Ollama servers in parallel exhausts system resources (CPU/RAM)
#
# Per "dont tolerate silent failing" directive: Tests MUST fail loudly if
# dependencies are unavailable, never silently skip.

# Fix(issue-double-spawn-001): Removed terminate-after from default profile
# Root cause: Nextest retry mechanism (terminate-after=2) causes double-spawn errors when current_dir != workspace_root
# When tests are executed with current_dir=/path/to/crate but binaries are in /path/to/workspace/target/debug/deps,
# the retry attempt fails with "No such file or directory" because path resolution is relative to current_dir
# Pitfall: terminate-after is intended for flaky test retries but incompatible with workspace/crate directory mismatch
[ profile.default ]
# Default timeout for regular unit tests (3 minutes)
slow-timeout = { period = "180s" }  # Removed: terminate-after = 2

# Fix(issue-double-spawn-002): Reduced global test-threads from 8 to 1
# Root cause: High parallelism (8 threads) + crate-level current_dir causes nextest path resolution failures
# When running from crate dir (/path/to/crate) with test binaries in workspace target (/path/to/workspace/target/debug/deps),
# nextest's parallel execution triggers double-spawn errors: "No such file or directory (os error 2)"
# Running from workspace root works (all 413 tests pass), but local nextest mode runs from crate dir
# Pitfall: Parallelism >1 exposes path resolution race conditions in workspace/crate directory mismatch scenarios
test-threads = 1  # Serialize all test binaries to avoid double-spawn errors

# Integration tests need extended timeouts for model pulling and server startup
# Each test binary gets unique Ollama server port via binary name hash
# Fix(issue-model-validation-timeout-001): Added example_model_validation_test to integration tests
# Root cause: test_chat_with_valid_model makes real chat request taking 360+ seconds, exceeded default 180s timeout
# Fix(issue-resource-exhaustion-002): Reduced from 2 to 1 parallel test binaries
# Root cause: Each test binary spawns Ollama server + loads model into RAM (1.4GB+ per server)
# With 2 parallel: 2.8GB+ RAM just for models, plus server overhead, causes system thrashing
# Model loading takes 60-180s - running 2 in parallel doubles resource pressure without time savings
# Fix(issue-model-loading-timeout-001): Added workspace_tests to integration test timeout overrides
# Root cause: All test binaries using Ollama need 180s+ for model loading, but only api_comprehensive_tests had extended timeout
# Workspace tests also make real API calls but were using default 180s timeout
# Pitfall: Any test binary making real Ollama requests needs extended timeout to handle model loading overhead
# Fix(issue-double-spawn-001): Removed terminate-after from integration test overrides
# Same root cause as default profile - retry mechanism incompatible with workspace/crate path resolution
# Fix(issue-invalid-config-001): Removed test-threads from overrides section
# Root cause: nextest only supports test-threads at profile level, not in overrides
# Warning: "ignoring unknown configuration keys: profile.default.overrides.N.test-threads"
# Global test-threads = 1 already controls binary parallelism; overrides can only set timeout and threads-required
# Pitfall: test-threads in overrides is silently ignored - use profile-level setting instead
[[ profile.default.overrides ]]
filter = 'package(api_ollama) and (binary(embeddings_tests) or binary(api_comprehensive_tests) or binary(streaming_tests) or binary(tool_calling_tests) or binary(vision_support_tests) or binary(example_model_validation_test) or binary(health_checks_tests) or binary(sync_api_tests) or binary(workspace_tests))'
slow-timeout = { period = "600s" }  # 10 minutes for integration tests (removed: terminate-after = 2)
# Note: test-threads controlled globally at profile level (line 30)
threads-required = "num-test-threads"  # Tests within binary run serially (share same Ollama server)

# Builder pattern tests use chat endpoint which is extremely slow with tinyllama (up to 12+ min per request)
# Fix(issue-builder-timeout-001): Increased from 600s -> 720s -> 750s -> 780s to handle slow chat endpoint responses
# Root cause: Chat endpoint with tinyllama takes 720s+ per request under resource constraints
# Observed: test_chat_request_builder_with_options passed at 361s, test_builder_complex_conversation failed at 741s
# Complex multi-message conversations (4+ messages) take 2x longer than simple requests due to context processing
# Fix(issue-resource-exhaustion-002): Reduced from 2 to 1 parallel executions
# Root cause: Same as integration tests - running multiple Ollama servers causes resource exhaustion
# Fix(issue-double-spawn-001): Removed terminate-after from builder test overrides
# Pitfall: Chat endpoint is much slower than generate endpoint - budget 12+ minutes per complex chat test
[[ profile.default.overrides ]]
filter = 'package(api_ollama) and binary(builder_patterns_tests)'
slow-timeout = { period = "780s" }  # 13 minutes for extremely slow chat tests (removed: terminate-after = 2)
# Note: test-threads controlled globally at profile level (line 30), not per-override
threads-required = "num-test-threads"
